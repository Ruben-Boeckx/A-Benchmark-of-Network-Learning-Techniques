{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import IMDB\n",
    "from torch_geometric.nn import HANConv\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "from itertools import product\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import (\n",
    "    HeteroData,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDB(InMemoryDataset):\n",
    "    r\"\"\"A subset of the Internet Movie Database (IMDB), as collected in the\n",
    "    `\"MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph\n",
    "    Embedding\" <https://arxiv.org/abs/2002.01680>`_ paper.\n",
    "    IMDB is a heterogeneous graph containing three types of entities - movies\n",
    "    (4,278 nodes), actors (5,257 nodes), and directors (2,081 nodes).\n",
    "    The movies are divided into three classes (action, comedy, drama) according\n",
    "    to their genre.\n",
    "    Movie features correspond to elements of a bag-of-words representation of\n",
    "    its plot keywords.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            every access. (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.HeteroData` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        force_reload (bool, optional): Whether to re-process the dataset.\n",
    "            (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    url = 'https://www.dropbox.com/s/g0btk9ctr1es39x/IMDB_processed.zip?dl=1'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "        force_reload: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(root, transform, pre_transform,\n",
    "                         force_reload=force_reload)\n",
    "        self.load(self.processed_paths[0], data_cls=HeteroData)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return [\n",
    "            'adjM.npz', 'features_0.npz', 'features_1.npz', 'features_2.npz',\n",
    "            'labels.npy', 'train_val_test_idx.npz'\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self) -> None:\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.remove(path)\n",
    "\n",
    "    def process(self) -> None:\n",
    "        data = HeteroData()\n",
    "\n",
    "        node_types = ['movie', 'director', 'actor']\n",
    "        for i, node_type in enumerate(node_types):\n",
    "            x = sp.load_npz(osp.join(self.raw_dir, f'features_{i}.npz'))\n",
    "            data[node_type].x = torch.from_numpy(x.todense()).to(torch.float)\n",
    "\n",
    "        y = np.load(osp.join(self.raw_dir, 'labels.npy'))\n",
    "        data['movie'].y = torch.from_numpy(y).to(torch.long)\n",
    "\n",
    "        split = np.load(osp.join(self.raw_dir, 'train_val_test_idx.npz'))\n",
    "        for name in ['train', 'val', 'test']:\n",
    "            idx = split[f'{name}_idx']\n",
    "            idx = torch.from_numpy(idx).to(torch.long)\n",
    "            mask = torch.zeros(data['movie'].num_nodes, dtype=torch.bool)\n",
    "            mask[idx] = True\n",
    "            data['movie'][f'{name}_mask'] = mask\n",
    "\n",
    "        s = {}\n",
    "        N_m = data['movie'].num_nodes\n",
    "        N_d = data['director'].num_nodes\n",
    "        N_a = data['actor'].num_nodes\n",
    "        s['movie'] = (0, N_m)\n",
    "        s['director'] = (N_m, N_m + N_d)\n",
    "        s['actor'] = (N_m + N_d, N_m + N_d + N_a)\n",
    "\n",
    "        A = sp.load_npz(osp.join(self.raw_dir, 'adjM.npz'))\n",
    "        for src, dst in product(node_types, node_types):\n",
    "            A_sub = A[s[src][0]:s[src][1], s[dst][0]:s[dst][1]].tocoo()\n",
    "            if A_sub.nnz > 0:\n",
    "                row = torch.from_numpy(A_sub.row).to(torch.long)\n",
    "                col = torch.from_numpy(A_sub.col).to(torch.long)\n",
    "                data[src, dst].edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        self.save([data], self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/g0btk9ctr1es39x/IMDB_processed.zip?dl=1\n",
      "Extracting data\\IMDB\\raw\\IMDB_processed.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "IMDB = IMDB(root='data/IMDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  movie={\n",
      "    x=[4278, 3066],\n",
      "    y=[4278],\n",
      "    train_mask=[4278],\n",
      "    val_mask=[4278],\n",
      "    test_mask=[4278],\n",
      "  },\n",
      "  director={ x=[2081, 3066] },\n",
      "  actor={ x=[5257, 3066] },\n",
      "  (movie, to, director)={ edge_index=[2, 4278] },\n",
      "  (movie, to, actor)={ edge_index=[2, 12828] },\n",
      "  (director, to, movie)={ edge_index=[2, 4278] },\n",
      "  (actor, to, movie)={ edge_index=[2, 12828] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data = IMDB[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "    def __init__(self, in_channels: Union[int, Dict[str, int]],\n",
    "                 out_channels: int, hidden_channels=128, heads=8):\n",
    "        super().__init__()\n",
    "        self.han_conv = HANConv(in_channels, hidden_channels, heads=heads,\n",
    "                                dropout=0.6, metadata=data.metadata())\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        out = self.han_conv(x_dict, edge_index_dict)\n",
    "        out = self.lin(out['movie'])\n",
    "        return out\n",
    "\n",
    "\n",
    "model = HAN(in_channels=-1, out_channels=3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data, model = data.to(device), model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train() -> float:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    mask = data['movie'].train_mask\n",
    "    loss = F.cross_entropy(out[mask], data['movie'].y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test() -> List[float]:\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "        mask = data['movie'][split]\n",
    "        acc = (pred[mask] == data['movie'].y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.0978, Train: 0.5150, Val: 0.3975, Test: 0.3695\n",
      "Epoch: 002, Loss: 1.0501, Train: 0.5800, Val: 0.4125, Test: 0.3884\n",
      "Epoch: 003, Loss: 0.9875, Train: 0.6625, Val: 0.4450, Test: 0.4080\n",
      "Epoch: 004, Loss: 0.9090, Train: 0.7250, Val: 0.4675, Test: 0.4313\n",
      "Epoch: 005, Loss: 0.8382, Train: 0.7850, Val: 0.5075, Test: 0.4635\n",
      "Epoch: 006, Loss: 0.7410, Train: 0.8450, Val: 0.5600, Test: 0.4957\n",
      "Epoch: 007, Loss: 0.6651, Train: 0.9000, Val: 0.5575, Test: 0.5221\n",
      "Epoch: 008, Loss: 0.5715, Train: 0.9500, Val: 0.5825, Test: 0.5454\n",
      "Epoch: 009, Loss: 0.5001, Train: 0.9675, Val: 0.5950, Test: 0.5719\n",
      "Epoch: 010, Loss: 0.4377, Train: 0.9800, Val: 0.6100, Test: 0.5788\n",
      "Epoch: 011, Loss: 0.3718, Train: 0.9850, Val: 0.5950, Test: 0.5802\n",
      "Epoch: 012, Loss: 0.3427, Train: 0.9850, Val: 0.6150, Test: 0.5802\n",
      "Epoch: 013, Loss: 0.2845, Train: 0.9875, Val: 0.6175, Test: 0.5828\n",
      "Epoch: 014, Loss: 0.2504, Train: 0.9950, Val: 0.6125, Test: 0.5805\n",
      "Epoch: 015, Loss: 0.2135, Train: 0.9950, Val: 0.6100, Test: 0.5791\n",
      "Epoch: 016, Loss: 0.2038, Train: 0.9950, Val: 0.6150, Test: 0.5817\n",
      "Epoch: 017, Loss: 0.1852, Train: 0.9950, Val: 0.6225, Test: 0.5782\n",
      "Epoch: 018, Loss: 0.1844, Train: 0.9950, Val: 0.6200, Test: 0.5750\n",
      "Epoch: 019, Loss: 0.1708, Train: 1.0000, Val: 0.6200, Test: 0.5794\n",
      "Epoch: 020, Loss: 0.1603, Train: 1.0000, Val: 0.6250, Test: 0.5779\n",
      "Epoch: 021, Loss: 0.1504, Train: 1.0000, Val: 0.6325, Test: 0.5794\n",
      "Epoch: 022, Loss: 0.1355, Train: 1.0000, Val: 0.6275, Test: 0.5791\n",
      "Epoch: 023, Loss: 0.1540, Train: 1.0000, Val: 0.6200, Test: 0.5779\n",
      "Epoch: 024, Loss: 0.1337, Train: 1.0000, Val: 0.6200, Test: 0.5727\n",
      "Epoch: 025, Loss: 0.1282, Train: 1.0000, Val: 0.6275, Test: 0.5730\n",
      "Epoch: 026, Loss: 0.1425, Train: 1.0000, Val: 0.6275, Test: 0.5713\n",
      "Epoch: 027, Loss: 0.1319, Train: 1.0000, Val: 0.6225, Test: 0.5684\n",
      "Epoch: 028, Loss: 0.1276, Train: 1.0000, Val: 0.6325, Test: 0.5702\n",
      "Epoch: 029, Loss: 0.1354, Train: 1.0000, Val: 0.6225, Test: 0.5733\n",
      "Epoch: 030, Loss: 0.1221, Train: 0.9975, Val: 0.6175, Test: 0.5736\n",
      "Epoch: 031, Loss: 0.1213, Train: 0.9975, Val: 0.6175, Test: 0.5725\n",
      "Epoch: 032, Loss: 0.1108, Train: 1.0000, Val: 0.6300, Test: 0.5699\n",
      "Epoch: 033, Loss: 0.1174, Train: 1.0000, Val: 0.6300, Test: 0.5725\n",
      "Epoch: 034, Loss: 0.1192, Train: 1.0000, Val: 0.6200, Test: 0.5699\n",
      "Epoch: 035, Loss: 0.1135, Train: 1.0000, Val: 0.6075, Test: 0.5647\n",
      "Epoch: 036, Loss: 0.1080, Train: 1.0000, Val: 0.6075, Test: 0.5633\n",
      "Epoch: 037, Loss: 0.1061, Train: 1.0000, Val: 0.6025, Test: 0.5661\n",
      "Epoch: 038, Loss: 0.1098, Train: 0.9975, Val: 0.6000, Test: 0.5704\n",
      "Epoch: 039, Loss: 0.1090, Train: 0.9975, Val: 0.5975, Test: 0.5768\n",
      "Epoch: 040, Loss: 0.1139, Train: 0.9975, Val: 0.6050, Test: 0.5736\n",
      "Epoch: 041, Loss: 0.1064, Train: 1.0000, Val: 0.6075, Test: 0.5702\n",
      "Epoch: 042, Loss: 0.0974, Train: 1.0000, Val: 0.6000, Test: 0.5578\n",
      "Epoch: 043, Loss: 0.1044, Train: 1.0000, Val: 0.6050, Test: 0.5541\n",
      "Epoch: 044, Loss: 0.0881, Train: 1.0000, Val: 0.6000, Test: 0.5543\n",
      "Epoch: 045, Loss: 0.0952, Train: 1.0000, Val: 0.6025, Test: 0.5592\n",
      "Epoch: 046, Loss: 0.0882, Train: 1.0000, Val: 0.6050, Test: 0.5681\n",
      "Epoch: 047, Loss: 0.0969, Train: 1.0000, Val: 0.6200, Test: 0.5730\n",
      "Epoch: 048, Loss: 0.0868, Train: 1.0000, Val: 0.6075, Test: 0.5727\n",
      "Epoch: 049, Loss: 0.1072, Train: 1.0000, Val: 0.5950, Test: 0.5722\n",
      "Epoch: 050, Loss: 0.0932, Train: 1.0000, Val: 0.5975, Test: 0.5653\n",
      "Epoch: 051, Loss: 0.0954, Train: 1.0000, Val: 0.6075, Test: 0.5696\n",
      "Epoch: 052, Loss: 0.0947, Train: 1.0000, Val: 0.6150, Test: 0.5633\n",
      "Epoch: 053, Loss: 0.0891, Train: 1.0000, Val: 0.6250, Test: 0.5621\n",
      "Epoch: 054, Loss: 0.0951, Train: 1.0000, Val: 0.6125, Test: 0.5679\n",
      "Epoch: 055, Loss: 0.0896, Train: 1.0000, Val: 0.6200, Test: 0.5756\n",
      "Epoch: 056, Loss: 0.0785, Train: 1.0000, Val: 0.6050, Test: 0.5742\n",
      "Epoch: 057, Loss: 0.0824, Train: 1.0000, Val: 0.6125, Test: 0.5704\n",
      "Epoch: 058, Loss: 0.0923, Train: 1.0000, Val: 0.6150, Test: 0.5690\n",
      "Epoch: 059, Loss: 0.0701, Train: 1.0000, Val: 0.6100, Test: 0.5607\n",
      "Epoch: 060, Loss: 0.0758, Train: 1.0000, Val: 0.6025, Test: 0.5578\n",
      "Epoch: 061, Loss: 0.0751, Train: 1.0000, Val: 0.6000, Test: 0.5584\n",
      "Epoch: 062, Loss: 0.0866, Train: 1.0000, Val: 0.6100, Test: 0.5687\n",
      "Epoch: 063, Loss: 0.0837, Train: 1.0000, Val: 0.6100, Test: 0.5670\n",
      "Epoch: 064, Loss: 0.0805, Train: 1.0000, Val: 0.6000, Test: 0.5687\n",
      "Epoch: 065, Loss: 0.0817, Train: 1.0000, Val: 0.6100, Test: 0.5687\n",
      "Epoch: 066, Loss: 0.0859, Train: 1.0000, Val: 0.5975, Test: 0.5641\n",
      "Epoch: 067, Loss: 0.0722, Train: 1.0000, Val: 0.6025, Test: 0.5569\n",
      "Epoch: 068, Loss: 0.0770, Train: 1.0000, Val: 0.6125, Test: 0.5630\n",
      "Epoch: 069, Loss: 0.0838, Train: 1.0000, Val: 0.6175, Test: 0.5704\n",
      "Epoch: 070, Loss: 0.0719, Train: 1.0000, Val: 0.6175, Test: 0.5802\n",
      "Epoch: 071, Loss: 0.0741, Train: 1.0000, Val: 0.6125, Test: 0.5771\n",
      "Epoch: 072, Loss: 0.0876, Train: 1.0000, Val: 0.6175, Test: 0.5730\n",
      "Epoch: 073, Loss: 0.0747, Train: 1.0000, Val: 0.6175, Test: 0.5704\n",
      "Epoch: 074, Loss: 0.0703, Train: 1.0000, Val: 0.6150, Test: 0.5716\n",
      "Epoch: 075, Loss: 0.0650, Train: 1.0000, Val: 0.6150, Test: 0.5667\n",
      "Epoch: 076, Loss: 0.0645, Train: 1.0000, Val: 0.6075, Test: 0.5656\n",
      "Epoch: 077, Loss: 0.0692, Train: 1.0000, Val: 0.6125, Test: 0.5676\n",
      "Epoch: 078, Loss: 0.0743, Train: 1.0000, Val: 0.6100, Test: 0.5702\n",
      "Epoch: 079, Loss: 0.0777, Train: 0.9975, Val: 0.6000, Test: 0.5713\n",
      "Epoch: 080, Loss: 0.0765, Train: 1.0000, Val: 0.5975, Test: 0.5667\n",
      "Epoch: 081, Loss: 0.0771, Train: 1.0000, Val: 0.6025, Test: 0.5630\n",
      "Epoch: 082, Loss: 0.0806, Train: 1.0000, Val: 0.5875, Test: 0.5610\n",
      "Epoch: 083, Loss: 0.0732, Train: 1.0000, Val: 0.6100, Test: 0.5644\n",
      "Epoch: 084, Loss: 0.0804, Train: 1.0000, Val: 0.6075, Test: 0.5653\n",
      "Epoch: 085, Loss: 0.0693, Train: 1.0000, Val: 0.6075, Test: 0.5653\n",
      "Epoch: 086, Loss: 0.0591, Train: 1.0000, Val: 0.6100, Test: 0.5615\n",
      "Epoch: 087, Loss: 0.0644, Train: 1.0000, Val: 0.6150, Test: 0.5650\n",
      "Epoch: 088, Loss: 0.0678, Train: 1.0000, Val: 0.6100, Test: 0.5704\n",
      "Epoch: 089, Loss: 0.0727, Train: 1.0000, Val: 0.6225, Test: 0.5684\n",
      "Epoch: 090, Loss: 0.0660, Train: 1.0000, Val: 0.6175, Test: 0.5687\n",
      "Epoch: 091, Loss: 0.0649, Train: 1.0000, Val: 0.6200, Test: 0.5719\n",
      "Epoch: 092, Loss: 0.0683, Train: 1.0000, Val: 0.6175, Test: 0.5684\n",
      "Epoch: 093, Loss: 0.0675, Train: 1.0000, Val: 0.6275, Test: 0.5670\n",
      "Epoch: 094, Loss: 0.0666, Train: 1.0000, Val: 0.6150, Test: 0.5676\n",
      "Epoch: 095, Loss: 0.0591, Train: 1.0000, Val: 0.6150, Test: 0.5681\n",
      "Epoch: 096, Loss: 0.0634, Train: 1.0000, Val: 0.6175, Test: 0.5719\n",
      "Epoch: 097, Loss: 0.0629, Train: 1.0000, Val: 0.6100, Test: 0.5693\n",
      "Epoch: 098, Loss: 0.0682, Train: 1.0000, Val: 0.6100, Test: 0.5687\n",
      "Epoch: 099, Loss: 0.0684, Train: 1.0000, Val: 0.6050, Test: 0.5612\n",
      "Epoch: 100, Loss: 0.0575, Train: 1.0000, Val: 0.6200, Test: 0.5601\n",
      "Epoch: 101, Loss: 0.0784, Train: 1.0000, Val: 0.6225, Test: 0.5558\n",
      "Epoch: 102, Loss: 0.0676, Train: 1.0000, Val: 0.6225, Test: 0.5575\n",
      "Epoch: 103, Loss: 0.0651, Train: 1.0000, Val: 0.6075, Test: 0.5635\n",
      "Epoch: 104, Loss: 0.0733, Train: 1.0000, Val: 0.6050, Test: 0.5656\n",
      "Epoch: 105, Loss: 0.0661, Train: 1.0000, Val: 0.5975, Test: 0.5644\n",
      "Epoch: 106, Loss: 0.0683, Train: 1.0000, Val: 0.6000, Test: 0.5635\n",
      "Epoch: 107, Loss: 0.0628, Train: 1.0000, Val: 0.6050, Test: 0.5572\n",
      "Epoch: 108, Loss: 0.0755, Train: 1.0000, Val: 0.6125, Test: 0.5575\n",
      "Epoch: 109, Loss: 0.0570, Train: 1.0000, Val: 0.6100, Test: 0.5575\n",
      "Epoch: 110, Loss: 0.0683, Train: 1.0000, Val: 0.6100, Test: 0.5673\n",
      "Epoch: 111, Loss: 0.0780, Train: 1.0000, Val: 0.5975, Test: 0.5684\n",
      "Epoch: 112, Loss: 0.0644, Train: 1.0000, Val: 0.6075, Test: 0.5707\n",
      "Epoch: 113, Loss: 0.0556, Train: 0.9975, Val: 0.6050, Test: 0.5736\n",
      "Epoch: 114, Loss: 0.0665, Train: 0.9975, Val: 0.5975, Test: 0.5739\n",
      "Epoch: 115, Loss: 0.0672, Train: 1.0000, Val: 0.5975, Test: 0.5673\n",
      "Epoch: 116, Loss: 0.0621, Train: 1.0000, Val: 0.5975, Test: 0.5656\n",
      "Epoch: 117, Loss: 0.0608, Train: 1.0000, Val: 0.5950, Test: 0.5644\n",
      "Epoch: 118, Loss: 0.0628, Train: 1.0000, Val: 0.5925, Test: 0.5679\n",
      "Epoch: 119, Loss: 0.0607, Train: 1.0000, Val: 0.5950, Test: 0.5656\n",
      "Epoch: 120, Loss: 0.0621, Train: 1.0000, Val: 0.5925, Test: 0.5592\n",
      "Epoch: 121, Loss: 0.0624, Train: 1.0000, Val: 0.5800, Test: 0.5633\n",
      "Epoch: 122, Loss: 0.0651, Train: 1.0000, Val: 0.5875, Test: 0.5604\n",
      "Epoch: 123, Loss: 0.0681, Train: 1.0000, Val: 0.5950, Test: 0.5635\n",
      "Epoch: 124, Loss: 0.0599, Train: 1.0000, Val: 0.5800, Test: 0.5612\n",
      "Epoch: 125, Loss: 0.0627, Train: 1.0000, Val: 0.5925, Test: 0.5650\n",
      "Epoch: 126, Loss: 0.0593, Train: 1.0000, Val: 0.6025, Test: 0.5630\n",
      "Epoch: 127, Loss: 0.0713, Train: 1.0000, Val: 0.5925, Test: 0.5601\n",
      "Epoch: 128, Loss: 0.0693, Train: 1.0000, Val: 0.5925, Test: 0.5592\n",
      "Stopping training as validation accuracy did not improve for 100 epochs\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "start_patience = patience = 100\n",
    "for epoch in range(1, 200):\n",
    "\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "    if best_val_acc <= val_acc:\n",
    "        patience = start_patience\n",
    "        best_val_acc = val_acc\n",
    "    else:\n",
    "        patience -= 1\n",
    "\n",
    "    if patience <= 0:\n",
    "        print('Stopping training as validation accuracy did not improve '\n",
    "              f'for {start_patience} epochs')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
